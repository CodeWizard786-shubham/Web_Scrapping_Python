{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website Name : College Dunia \\\n",
    "WebSite to Scrape: https://collegedunia.com/india-colleges?custom_params=%5Bview%3Atable%5D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources : \n",
    "1. Python 3.8\n",
    "2. Selenium v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import time\n",
    "import csv\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('selenium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and access chrome driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access Website URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_url():\n",
    "    try:\n",
    "        url = 'https://collegedunia.com/india-colleges?custom_params=%5Bview%3Atable%5D'\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        logger.error(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapping college records to store csv file simultaneously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record_file_path =  \"D:\\\\record_count.txt\"\n",
    "\n",
    "def get_record_count():\n",
    "    # Check if the record_count.txt file exists\n",
    "    if os.path.exists(record_file_path):\n",
    "        # If it exists, read the current record count from the file\n",
    "        with open(record_file_path, 'r') as file:\n",
    "            record_count = int(file.read())\n",
    "        return record_count\n",
    "    else:\n",
    "        # If it doesn't exist, start from the beginning (record_count = 0)\n",
    "        return 0\n",
    "\n",
    "def update_record_count(count):   \n",
    "    # Update the record count in the record_count.txt file\n",
    "    with open(record_file_path, 'w') as file:\n",
    "        file.write(str(count))\n",
    "\n",
    "def scrape_data():\n",
    "    try:\n",
    "        start_time = time()\n",
    "        access_url()\n",
    "        table = driver.find_element(By.TAG_NAME, value=\"thead\")\n",
    "        entries = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "        headers = [th.text for th in entries[0].find_elements(By.TAG_NAME, \"th\")]\n",
    "        # store fetched records directly in csv file\n",
    "        with open('college_dunia.csv', mode='a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            # Check the record count from the record_count.txt file\n",
    "            record_count = get_record_count()\n",
    "            if record_count == 0:\n",
    "                writer.writerow(headers)\n",
    "            stored_records = set()\n",
    "            final_element = \"jsx-2796823646.jsx-1342907234.endOfContainer\"\n",
    "            while final_element not in driver.page_source:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", driver.find_element(By.CLASS_NAME, final_element))\n",
    "                driver.implicitly_wait(2)\n",
    "\n",
    "                # Using XPath to get all <tr> elements in the <tbody>\n",
    "                entries = driver.find_elements(By.XPATH, \"//tbody/tr\")\n",
    "\n",
    "                for entry in entries[record_count:]:\n",
    "                    data_rows = [td.text.strip().replace(\"\\n\", \" \") for td in entry.find_elements(By.XPATH, \"./td\")]\n",
    "                    data_rows = [row for row in data_rows if row]\n",
    "                    if data_rows: \n",
    "                        record = '\\t'.join(data_rows)\n",
    "                        if record not in stored_records: \n",
    "                            stored_records.add(record)\n",
    "                            if '#' in data_rows[0]: \n",
    "                                writer.writerow(data_rows)\n",
    "                            \n",
    "\n",
    "                    record_count += 1\n",
    "\n",
    "                # Update the record count in the record_count.txt file\n",
    "                update_record_count(record_count)\n",
    "\n",
    "                if final_element in driver.page_source:\n",
    "                    stop_time = time()\n",
    "                    break\n",
    "\n",
    "        driver.close()\n",
    "        print(\"Data Scrapped in data.csv file\")\n",
    "        total_elapsed_time = (stop_time - start_time) / 60\n",
    "        return total_elapsed_time\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred: \", str(e))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the scrape function\n",
    "total_elapsed_time = scrape_data()\n",
    "print(\"Web scrapping sucessfully completed\")\n",
    "print(f\"Total scrape Time taken:{total_elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Data in hadoop in distributed manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"10.0.2.15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    # create spark session\n",
    "    spark = SparkSession.builder.master('local').appName(\"CollegeDuniaRecords\").getOrCreate() \n",
    "\n",
    "    input_file_path = '/home/hdoop/Documents/python/python-repo/WebScraping/college_dunia.csv'\n",
    "    # Read csv\n",
    "    df = spark.read.format(\"csv\").options(inferSchema=\"True\", sep=\",\", header=\"True\").load(input_file_path)\n",
    "    # write csv file to hadoop filesystem\n",
    "    df.write.format(\"csv\").save(\"hdfs://localhost:9000/web_scrape_data/college_dunia_records.csv\",header=True,inferschema=True)\n",
    "    \n",
    "    print(\"Web scraped data Sucessfully Loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
